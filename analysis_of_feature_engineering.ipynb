{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn import linear_model\n",
    "from sklearn import svm\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt\n",
    "import skfeature\n",
    "from scipy import stats\n",
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# An Introduction to Feature Engineering\n",
    "\n",
    "By Eric Schles\n",
    "\n",
    "Today we are going to cover feature engineering.  We'll start with the easiest form of feature engineering - creating dummy variables.  And then we'll move onto taking the log of diffierent variables, taking the variable and the square of variables, adding the results of other models as features to the model, and finally automatic feature selection, via skfeature and sklearn.  \n",
    "\n",
    "To summarize we'll cover:\n",
    "\n",
    "* dummy variables\n",
    "* logs of variables\n",
    "* boolean values\n",
    "* multiplying variables\n",
    "* features from exogenous data\n",
    "* the outputs of other models as features\n",
    "* automatic feature selection via skfeature\n",
    "* automatic feature selection via PCA\n",
    "* automatic feature selection via TSNE\n",
    "* automatic feature selection via DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first let's import some data\n",
    "\n",
    "train = pd.read_csv(\"train.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A brief introduction to Dummy variables\n",
    "\n",
    "A dummy variable is a transformation of a categorical variable into a numeric variable.  This allows us to process a whole new set of data points, that we couldn't before.  However, there are some dangers.  We shouldn't interpret dummy variables as truly numeric, because there isn't necessarily a meaning to the transformation we choose.  As long as we map to variables that don't have large magnitudinal differences, and there are only a few categories, dummy variables give us a lot of power.\n",
    "\n",
    "But if we have a ton of categories, a strict dummy variable mapping may be ill advised."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# starting with dummy variables\n",
    "# We'll want to pick dummary variables that occur a somewhat balanced number of times\n",
    "# that makes variables like the following poor choices:\n",
    "\n",
    "train[\"Street\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And variables like LotShape pretty good:\n",
    "train[\"LotShape\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Operationalizing dummy variables\n",
    "\n",
    "Now that we've figured out what a good dummy variable looks like, let's operationalize it with a function!  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dummy_variables(df, length_cut_off_percent = 0.75, max_concentration = 0.65):\n",
    "    candidate_columns = []\n",
    "    for column in df.columns:\n",
    "        if df[column].dtype == object:\n",
    "            candidate_columns.append(column)\n",
    "    \n",
    "    for column in candidate_columns:\n",
    "        value_counts = df[column].value_counts()\n",
    "        if len(value_counts) < len(df)*length_cut_off_percent: \n",
    "            candidate_columns.remove(column)\n",
    "            continue\n",
    "        sum_vals = sum(value_counts)\n",
    "        percentages = [elem/sum_vals > max_concentration for elem in value_counts]\n",
    "        if any(percentages):\n",
    "            candidate_columns.remove(column)\n",
    "        continue\n",
    "    \n",
    "    for column in candidate_columns:\n",
    "        dummy_columns = pd.get_dummies(df[column])\n",
    "        df = pd.concat([df, dummy_columns], axis=1)\n",
    "        df.drop(column, axis=1, inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now let's see which variables we are going to turn into dummies\n",
    "train_with_dummies = generate_dummy_variables(train)\n",
    "print(train.shape, train_with_dummies.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see we went from 83 columns to 175.  That's a lot more features!  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logarithmic transformation\n",
    "\n",
    "I'm not sure if logarithmic transformations are considered \"feature engineering\" however they are a powerful tool used often in econometrics to bring flexibility to the modeling of data.  In order to make use of and interpret this next technique we will need some information about our dataset.  Does it makes sense to take the log of any of the variables?\n",
    "\n",
    "One of the ways you can tell this, is if the dependent variable and the independent variables are not in the same scale.  Which is often the case when dealing with prices.  So let's go ahead and build a rather standard model with our data:\n",
    "\n",
    "`log(SalePrice) = B[0] + B[1]*LotArea + B[2]*BedroomAbvGr + B[3]*ExterQual + B[4]*OverallQual + B[5]*OverallCond + u`\n",
    "\n",
    "```\n",
    "SalePrice = the price the house was sold for \n",
    "LotArea = the size of the lot\n",
    "BedroomAbvGr = the number of bedrooms above basement level\n",
    "OveralQual = Rates the overall material and finish of the house\n",
    "OverallCond = Rates the overall condition of the house\n",
    "ExterQual = the quality of the exterior of the home (See Note)\n",
    "```\n",
    "\n",
    "Note - Even though these are categorical variables, there is an ordering to each variable Excellent, Good, Average, Fair and Poor.  So we can translate these variables into numeric ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping = {\n",
    "    \"Ex\": 5, # Excellent\n",
    "    \"Gd\": 4, # Good\n",
    "    \"TA\": 3, # Average/Typical\n",
    "    \"Fa\": 2, # Fair\n",
    "    \"Po\": 1 # Poor\n",
    "}\n",
    "train[\"ExterQual\"] = train[\"ExterQual\"].map(mapping)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"log_SalePrice\"] = np.log(train[\"SalePrice\"])\n",
    "\n",
    "sale_price = train[\"log_SalePrice\"]\n",
    "X = train[[\"LotArea\", \"BedroomAbvGr\", \"ExterQual\", \"OverallQual\", \"OverallCond\"]]\n",
    "X = sm.add_constant(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = sm.OLS(sale_price, X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_results = model.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>      <td>log_SalePrice</td>  <th>  R-squared:         </th> <td>   0.733</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.732</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   797.1</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Wed, 09 May 2018</td> <th>  Prob (F-statistic):</th>  <td>  0.00</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>06:49:56</td>     <th>  Log-Likelihood:    </th> <td>  231.80</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>  1460</td>      <th>  AIC:               </th> <td>  -451.6</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>  1454</td>      <th>  BIC:               </th> <td>  -419.9</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     5</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "        <td></td>          <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th>        <td>   10.0387</td> <td>    0.050</td> <td>  202.579</td> <td> 0.000</td> <td>    9.941</td> <td>   10.136</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>LotArea</th>      <td> 6.529e-06</td> <td> 5.49e-07</td> <td>   11.890</td> <td> 0.000</td> <td> 5.45e-06</td> <td> 7.61e-06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>BedroomAbvGr</th> <td>    0.0631</td> <td>    0.007</td> <td>    9.312</td> <td> 0.000</td> <td>    0.050</td> <td>    0.076</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>ExterQual</th>    <td>    0.1537</td> <td>    0.014</td> <td>   11.040</td> <td> 0.000</td> <td>    0.126</td> <td>    0.181</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>OverallQual</th>  <td>    0.1823</td> <td>    0.006</td> <td>   31.444</td> <td> 0.000</td> <td>    0.171</td> <td>    0.194</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>OverallCond</th>  <td>    0.0183</td> <td>    0.005</td> <td>    3.731</td> <td> 0.000</td> <td>    0.009</td> <td>    0.028</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>210.533</td> <th>  Durbin-Watson:     </th> <td>   1.942</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td> 632.285</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td>-0.733</td>  <th>  Prob(JB):          </th> <td>5.02e-138</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 5.871</td>  <th>  Cond. No.          </th> <td>1.35e+05</td> \n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:          log_SalePrice   R-squared:                       0.733\n",
       "Model:                            OLS   Adj. R-squared:                  0.732\n",
       "Method:                 Least Squares   F-statistic:                     797.1\n",
       "Date:                Wed, 09 May 2018   Prob (F-statistic):               0.00\n",
       "Time:                        06:49:56   Log-Likelihood:                 231.80\n",
       "No. Observations:                1460   AIC:                            -451.6\n",
       "Df Residuals:                    1454   BIC:                            -419.9\n",
       "Df Model:                           5                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "================================================================================\n",
       "                   coef    std err          t      P>|t|      [0.025      0.975]\n",
       "--------------------------------------------------------------------------------\n",
       "const           10.0387      0.050    202.579      0.000       9.941      10.136\n",
       "LotArea       6.529e-06   5.49e-07     11.890      0.000    5.45e-06    7.61e-06\n",
       "BedroomAbvGr     0.0631      0.007      9.312      0.000       0.050       0.076\n",
       "ExterQual        0.1537      0.014     11.040      0.000       0.126       0.181\n",
       "OverallQual      0.1823      0.006     31.444      0.000       0.171       0.194\n",
       "OverallCond      0.0183      0.005      3.731      0.000       0.009       0.028\n",
       "==============================================================================\n",
       "Omnibus:                      210.533   Durbin-Watson:                   1.942\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              632.285\n",
       "Skew:                          -0.733   Prob(JB):                    5.02e-138\n",
       "Kurtosis:                       5.871   Cond. No.                     1.35e+05\n",
       "==============================================================================\n",
       "\n",
       "Warnings:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "[2] The condition number is large, 1.35e+05. This might indicate that there are\n",
       "strong multicollinearity or other numerical problems.\n",
       "\"\"\""
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_results.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding how to think about logs in linear models\n",
    "\n",
    "Generally speaking when one applies a log to the dependent variable of an OLS model, we are asking \"What is the elasticity of sale price with respect to lot area, number of above ground bedrooms, external quality of the material of the house, overall quality of the house and overall condition of the house?\"  Thinking of this another way, we can see this as the percentage change in price with respect to a change in a dependent variable.  \n",
    "\n",
    "Thus, we have put our dependent variable in relative, not absolute terms to our independent variables.  In terms of a percentage change, things can be misleading, if not well understood.  But when treated carefully, percentage changes can shed more light than just dealing with absolute numbers.  Especially when changes in Y appear insensitive to changes in X, due to scaling reasons.  \n",
    "\n",
    "This that in mind, let's look at our results!\n",
    "\n",
    "Looking at our model:\n",
    "\n",
    "We can say that Overall Quality has a large and statistically significant effect on the percentage change in the sale price for a house.  This seems to be the largest factor, with External Quality coming in second and number of bedrooms coming in third.  It's possible that higher quality materials are only used in larger more well cared for houses.  So it might be the case, that there is some correlation between the three variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.7262784907641429, 1.2901524338571525e-239)\n",
      "(0.10167635624178528, 9.949911037119167e-05)\n"
     ]
    }
   ],
   "source": [
    "print(stats.pearsonr(X[\"ExterQual\"], X[\"OverallQual\"]))\n",
    "print(stats.pearsonr(X[\"BedroomAbvGr\"], X[\"OverallQual\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unsurprisingly, there is a strong correlation between all three variables.  So we can drop ExterQual for sure.  We can also see what happens to the model statistics - R^2, AIC, BIC and the t-tests of specific variables, to determine if we should drop both variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>      <td>log_SalePrice</td>  <th>  R-squared:         </th> <td>   0.710</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.710</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   891.9</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Wed, 09 May 2018</td> <th>  Prob (F-statistic):</th>  <td>  0.00</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>06:59:33</td>     <th>  Log-Likelihood:    </th> <td>  173.04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>  1460</td>      <th>  AIC:               </th> <td>  -336.1</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>  1455</td>      <th>  BIC:               </th> <td>  -309.6</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     4</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "        <td></td>          <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th>        <td>   10.3368</td> <td>    0.043</td> <td>  239.067</td> <td> 0.000</td> <td>   10.252</td> <td>   10.422</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>LotArea</th>      <td>  6.43e-06</td> <td> 5.71e-07</td> <td>   11.253</td> <td> 0.000</td> <td> 5.31e-06</td> <td> 7.55e-06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>BedroomAbvGr</th> <td>    0.0533</td> <td>    0.007</td> <td>    7.621</td> <td> 0.000</td> <td>    0.040</td> <td>    0.067</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>OverallQual</th>  <td>    0.2289</td> <td>    0.004</td> <td>   55.369</td> <td> 0.000</td> <td>    0.221</td> <td>    0.237</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>OverallCond</th>  <td>    0.0127</td> <td>    0.005</td> <td>    2.503</td> <td> 0.012</td> <td>    0.003</td> <td>    0.023</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>179.440</td> <th>  Durbin-Watson:     </th> <td>   1.939</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td> 436.137</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td>-0.691</td>  <th>  Prob(JB):          </th> <td>1.97e-95</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 5.293</td>  <th>  Cond. No.          </th> <td>1.12e+05</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:          log_SalePrice   R-squared:                       0.710\n",
       "Model:                            OLS   Adj. R-squared:                  0.710\n",
       "Method:                 Least Squares   F-statistic:                     891.9\n",
       "Date:                Wed, 09 May 2018   Prob (F-statistic):               0.00\n",
       "Time:                        06:59:33   Log-Likelihood:                 173.04\n",
       "No. Observations:                1460   AIC:                            -336.1\n",
       "Df Residuals:                    1455   BIC:                            -309.6\n",
       "Df Model:                           4                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "================================================================================\n",
       "                   coef    std err          t      P>|t|      [0.025      0.975]\n",
       "--------------------------------------------------------------------------------\n",
       "const           10.3368      0.043    239.067      0.000      10.252      10.422\n",
       "LotArea        6.43e-06   5.71e-07     11.253      0.000    5.31e-06    7.55e-06\n",
       "BedroomAbvGr     0.0533      0.007      7.621      0.000       0.040       0.067\n",
       "OverallQual      0.2289      0.004     55.369      0.000       0.221       0.237\n",
       "OverallCond      0.0127      0.005      2.503      0.012       0.003       0.023\n",
       "==============================================================================\n",
       "Omnibus:                      179.440   Durbin-Watson:                   1.939\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              436.137\n",
       "Skew:                          -0.691   Prob(JB):                     1.97e-95\n",
       "Kurtosis:                       5.293   Cond. No.                     1.12e+05\n",
       "==============================================================================\n",
       "\n",
       "Warnings:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "[2] The condition number is large, 1.12e+05. This might indicate that there are\n",
       "strong multicollinearity or other numerical problems.\n",
       "\"\"\""
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = train[[\"LotArea\", \"BedroomAbvGr\", \"OverallQual\", \"OverallCond\"]]\n",
    "X = sm.add_constant(X)\n",
    "model = sm.OLS(sale_price, X)\n",
    "model_results = model.fit()\n",
    "model_results.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dropping ExterQual seems to have worked out nicely!  The R^2 doesn't fall much and none of the variables become statistically insignificant.  So we can safely drop ExterQual.\n",
    "\n",
    "Next let's see if we can drop BedroomAbvGr."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>      <td>log_SalePrice</td>  <th>  R-squared:         </th> <td>   0.699</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.698</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   1126.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Wed, 09 May 2018</td> <th>  Prob (F-statistic):</th>  <td>  0.00</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>07:01:03</td>     <th>  Log-Likelihood:    </th> <td>  144.47</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>  1460</td>      <th>  AIC:               </th> <td>  -280.9</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>  1456</td>      <th>  BIC:               </th> <td>  -259.8</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     3</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "       <td></td>          <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th>       <td>   10.4620</td> <td>    0.041</td> <td>  256.599</td> <td> 0.000</td> <td>   10.382</td> <td>   10.542</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>LotArea</th>     <td> 6.909e-06</td> <td> 5.79e-07</td> <td>   11.934</td> <td> 0.000</td> <td> 5.77e-06</td> <td> 8.04e-06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>OverallQual</th> <td>    0.2318</td> <td>    0.004</td> <td>   55.235</td> <td> 0.000</td> <td>    0.224</td> <td>    0.240</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>OverallCond</th> <td>    0.0136</td> <td>    0.005</td> <td>    2.622</td> <td> 0.009</td> <td>    0.003</td> <td>    0.024</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>176.718</td> <th>  Durbin-Watson:     </th> <td>   1.938</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td> 415.143</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td>-0.693</td>  <th>  Prob(JB):          </th> <td>7.13e-91</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 5.215</td>  <th>  Cond. No.          </th> <td>1.04e+05</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:          log_SalePrice   R-squared:                       0.699\n",
       "Model:                            OLS   Adj. R-squared:                  0.698\n",
       "Method:                 Least Squares   F-statistic:                     1126.\n",
       "Date:                Wed, 09 May 2018   Prob (F-statistic):               0.00\n",
       "Time:                        07:01:03   Log-Likelihood:                 144.47\n",
       "No. Observations:                1460   AIC:                            -280.9\n",
       "Df Residuals:                    1456   BIC:                            -259.8\n",
       "Df Model:                           3                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "===============================================================================\n",
       "                  coef    std err          t      P>|t|      [0.025      0.975]\n",
       "-------------------------------------------------------------------------------\n",
       "const          10.4620      0.041    256.599      0.000      10.382      10.542\n",
       "LotArea      6.909e-06   5.79e-07     11.934      0.000    5.77e-06    8.04e-06\n",
       "OverallQual     0.2318      0.004     55.235      0.000       0.224       0.240\n",
       "OverallCond     0.0136      0.005      2.622      0.009       0.003       0.024\n",
       "==============================================================================\n",
       "Omnibus:                      176.718   Durbin-Watson:                   1.938\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              415.143\n",
       "Skew:                          -0.693   Prob(JB):                     7.13e-91\n",
       "Kurtosis:                       5.215   Cond. No.                     1.04e+05\n",
       "==============================================================================\n",
       "\n",
       "Warnings:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "[2] The condition number is large, 1.04e+05. This might indicate that there are\n",
       "strong multicollinearity or other numerical problems.\n",
       "\"\"\""
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = train[[\"LotArea\", \"OverallQual\", \"OverallCond\"]]\n",
    "X = sm.add_constant(X)\n",
    "model = sm.OLS(sale_price, X)\n",
    "model_results = model.fit()\n",
    "model_results.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The R^2 didn't change much, but the AIC fell 56 points from -336 to -280.  So it might be worth including bedrooms in the model.  How it's best to look at alternative features before including ones with only marginal effects.  Also, the simplest sufficiently useful model is always the best."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Let's do one more experiment -\n",
    "\n",
    "What happens if we take the log with respect to LotArea?  A good reason to take the log of this variable is that it is likely much larger than the other variables under consideration.  And so a linear model cannot capture it well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"log_SalePrice\"] = np.log(train[\"SalePrice\"])\n",
    "train[\"log_LotArea\"] = np.log(train[\"LotArea\"])\n",
    "\n",
    "sale_price = train[\"log_SalePrice\"]\n",
    "X = train[[\"log_LotArea\", \"OverallQual\", \"OverallCond\"]]\n",
    "X = sm.add_constant(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>      <td>log_SalePrice</td>  <th>  R-squared:         </th> <td>   0.736</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.735</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   1352.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Wed, 09 May 2018</td> <th>  Prob (F-statistic):</th>  <td>  0.00</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>07:09:22</td>     <th>  Log-Likelihood:    </th> <td>  240.44</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>  1460</td>      <th>  AIC:               </th> <td>  -472.9</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>  1456</td>      <th>  BIC:               </th> <td>  -451.7</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     3</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "       <td></td>          <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th>       <td>    8.7446</td> <td>    0.099</td> <td>   88.094</td> <td> 0.000</td> <td>    8.550</td> <td>    8.939</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>log_LotArea</th> <td>    0.2024</td> <td>    0.011</td> <td>   19.158</td> <td> 0.000</td> <td>    0.182</td> <td>    0.223</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>OverallQual</th> <td>    0.2235</td> <td>    0.004</td> <td>   56.285</td> <td> 0.000</td> <td>    0.216</td> <td>    0.231</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>OverallCond</th> <td>    0.0129</td> <td>    0.005</td> <td>    2.655</td> <td> 0.008</td> <td>    0.003</td> <td>    0.022</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>233.333</td> <th>  Durbin-Watson:     </th> <td>   1.927</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td> 715.041</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td>-0.803</td>  <th>  Prob(JB):          </th> <td>5.38e-156</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 6.029</td>  <th>  Cond. No.          </th> <td>    230.</td> \n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:          log_SalePrice   R-squared:                       0.736\n",
       "Model:                            OLS   Adj. R-squared:                  0.735\n",
       "Method:                 Least Squares   F-statistic:                     1352.\n",
       "Date:                Wed, 09 May 2018   Prob (F-statistic):               0.00\n",
       "Time:                        07:09:22   Log-Likelihood:                 240.44\n",
       "No. Observations:                1460   AIC:                            -472.9\n",
       "Df Residuals:                    1456   BIC:                            -451.7\n",
       "Df Model:                           3                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "===============================================================================\n",
       "                  coef    std err          t      P>|t|      [0.025      0.975]\n",
       "-------------------------------------------------------------------------------\n",
       "const           8.7446      0.099     88.094      0.000       8.550       8.939\n",
       "log_LotArea     0.2024      0.011     19.158      0.000       0.182       0.223\n",
       "OverallQual     0.2235      0.004     56.285      0.000       0.216       0.231\n",
       "OverallCond     0.0129      0.005      2.655      0.008       0.003       0.022\n",
       "==============================================================================\n",
       "Omnibus:                      233.333   Durbin-Watson:                   1.927\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              715.041\n",
       "Skew:                          -0.803   Prob(JB):                    5.38e-156\n",
       "Kurtosis:                       6.029   Cond. No.                         230.\n",
       "==============================================================================\n",
       "\n",
       "Warnings:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = sm.OLS(sale_price, X)\n",
    "model_results = model.fit()\n",
    "model_results.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perhaps unsurprisingly, our overall model performance improves, and now log_LotArea has a reasonably large coefficient, at least compared to before we took the log.  But how to interpret this parameter now?\n",
    "\n",
    "Well, holding all other variables constant, a 1% change in log_LotArea leads to a (0.01)*0.2024 ~ 0.002 percent change in sale price.  Which appears small, but can actually be an amount of money worth consideration, depending on the house. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding Boolean Values\n",
    "\n",
    "Another way we can make our model better is by teasing out specific effects at certain levels.  Maybe a variable that's continuous should be categorical?  But how to do this?  Well there are a few major techniques:\n",
    "\n",
    "1. Singling out specific values - this is an equality boolean\n",
    "2. Setting thresholds - this is when you look to see if the given value is above or below a specific value\n",
    "3. High pass and low pass - this is when you look to see if a value is at it's extremes\n",
    "\n",
    "Let's consider our model and a new variable who's scale makes it a little hard to work with - `YearBuilt`.\n",
    "\n",
    "YearBuilt is the original construction date\n",
    "\n",
    "Before we can define an appropriate transformation we need to know what we are dealing with.  So let's do some exploratory analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2010, 1872)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(train[\"YearBuilt\"]), min(train[\"YearBuilt\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7fbec8d0ec50>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fbec8cd0198>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train[\"YearBuilt\"].plot.hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears as though more houses have been built overtime, or the survival rate for a given house falls over time.  In any event, it gives us a natural information criteria for variable transformation:\n",
    "\n",
    "\n",
    "`really_old := YearBuilt < 1920`\n",
    "\n",
    "`reasonably_old := YearBuilt >= 1920 and YearBuilt < 1960`\n",
    "\n",
    "`a_little_old := YearBuilt >= 1960 and YearBuilt < 1982`\n",
    "\n",
    "`somewhat_new := YearBuilt >= 1982 and YearBuilt < 2008`\n",
    "\n",
    "`brand_new := YearBuilt >= 2008`\n",
    "\n",
    "The reason 2008 and forward is considered brand new is because the data only goes until 2010 and it takes time to build a house."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reasonably_old(x):\n",
    "    if x[\"YearBuilt\"] >= 1920 and x[\"YearBuilt\"] < 1960:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "    \n",
    "def a_little_old(x):\n",
    "    if x[\"YearBuilt\"] >= 1960 and x[\"YearBuilt\"] < 1982:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def somewhat_new(x):\n",
    "    if x[\"YearBuilt\"] >= 1982 and x[\"YearBuilt\"] < 2008:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "boolean_map = {\n",
    "    True: 1,\n",
    "    False: 0\n",
    "}\n",
    "train[\"really_old\"] = train[\"YearBuilt\"] < 1920\n",
    "train[\"reasonably_old\"] = train.apply(reasonably_old, axis=1)\n",
    "train[\"a_little_old\"] = train.apply(a_little_old, axis=1)\n",
    "train[\"somewhat_new\"] = train.apply(somewhat_new, axis=1)\n",
    "train[\"brand_new\"] = train[\"YearBuilt\"] >= 2008\n",
    "\n",
    "train[\"really_old\"] = train[\"really_old\"].map(boolean_map)\n",
    "train[\"reasonably_old\"] = train[\"reasonably_old\"].map(boolean_map)\n",
    "train[\"a_little_old\"] = train[\"a_little_old\"].map(boolean_map)\n",
    "train[\"somewhat_new\"] = train[\"somewhat_new\"].map(boolean_map)\n",
    "train[\"brand_new\"] = train[\"brand_new\"].map(boolean_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"log_SalePrice\"] = np.log(train[\"SalePrice\"])\n",
    "train[\"log_LotArea\"] = np.log(train[\"LotArea\"])\n",
    "\n",
    "sale_price = train[\"log_SalePrice\"]\n",
    "X = train[[\"log_LotArea\", \n",
    "           \"OverallQual\", \n",
    "           \"OverallCond\", \n",
    "           \"really_old\", \n",
    "           \"reasonably_old\", \n",
    "           \"a_little_old\", \n",
    "           \"somewhat_new\", \n",
    "           \"brand_new\"]\n",
    "         ]\n",
    "X = sm.add_constant(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>      <td>log_SalePrice</td>  <th>  R-squared:         </th> <td>   0.773</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.772</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   704.7</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Wed, 09 May 2018</td> <th>  Prob (F-statistic):</th>  <td>  0.00</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>07:33:14</td>     <th>  Log-Likelihood:    </th> <td>  349.79</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>  1460</td>      <th>  AIC:               </th> <td>  -683.6</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>  1452</td>      <th>  BIC:               </th> <td>  -641.3</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     7</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "         <td></td>           <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th>          <td>    7.2543</td> <td>    0.077</td> <td>   93.623</td> <td> 0.000</td> <td>    7.102</td> <td>    7.406</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>log_LotArea</th>    <td>    0.2220</td> <td>    0.010</td> <td>   22.367</td> <td> 0.000</td> <td>    0.203</td> <td>    0.241</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>OverallQual</th>    <td>    0.1742</td> <td>    0.005</td> <td>   34.155</td> <td> 0.000</td> <td>    0.164</td> <td>    0.184</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>OverallCond</th>    <td>    0.0426</td> <td>    0.005</td> <td>    8.600</td> <td> 0.000</td> <td>    0.033</td> <td>    0.052</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>really_old</th>     <td>    1.2810</td> <td>    0.025</td> <td>   51.883</td> <td> 0.000</td> <td>    1.233</td> <td>    1.329</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>reasonably_old</th> <td>    1.3252</td> <td>    0.019</td> <td>   68.802</td> <td> 0.000</td> <td>    1.287</td> <td>    1.363</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>a_little_old</th>   <td>    1.4266</td> <td>    0.019</td> <td>   75.814</td> <td> 0.000</td> <td>    1.390</td> <td>    1.463</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>somewhat_new</th>   <td>    1.5583</td> <td>    0.018</td> <td>   85.372</td> <td> 0.000</td> <td>    1.522</td> <td>    1.594</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>brand_new</th>      <td>    1.6633</td> <td>    0.032</td> <td>   52.801</td> <td> 0.000</td> <td>    1.601</td> <td>    1.725</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>220.512</td> <th>  Durbin-Watson:     </th> <td>   1.937</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td> 918.531</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td>-0.666</td>  <th>  Prob(JB):          </th> <td>3.50e-200</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 6.650</td>  <th>  Cond. No.          </th> <td>2.10e+16</td> \n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:          log_SalePrice   R-squared:                       0.773\n",
       "Model:                            OLS   Adj. R-squared:                  0.772\n",
       "Method:                 Least Squares   F-statistic:                     704.7\n",
       "Date:                Wed, 09 May 2018   Prob (F-statistic):               0.00\n",
       "Time:                        07:33:14   Log-Likelihood:                 349.79\n",
       "No. Observations:                1460   AIC:                            -683.6\n",
       "Df Residuals:                    1452   BIC:                            -641.3\n",
       "Df Model:                           7                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==================================================================================\n",
       "                     coef    std err          t      P>|t|      [0.025      0.975]\n",
       "----------------------------------------------------------------------------------\n",
       "const              7.2543      0.077     93.623      0.000       7.102       7.406\n",
       "log_LotArea        0.2220      0.010     22.367      0.000       0.203       0.241\n",
       "OverallQual        0.1742      0.005     34.155      0.000       0.164       0.184\n",
       "OverallCond        0.0426      0.005      8.600      0.000       0.033       0.052\n",
       "really_old         1.2810      0.025     51.883      0.000       1.233       1.329\n",
       "reasonably_old     1.3252      0.019     68.802      0.000       1.287       1.363\n",
       "a_little_old       1.4266      0.019     75.814      0.000       1.390       1.463\n",
       "somewhat_new       1.5583      0.018     85.372      0.000       1.522       1.594\n",
       "brand_new          1.6633      0.032     52.801      0.000       1.601       1.725\n",
       "==============================================================================\n",
       "Omnibus:                      220.512   Durbin-Watson:                   1.937\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              918.531\n",
       "Skew:                          -0.666   Prob(JB):                    3.50e-200\n",
       "Kurtosis:                       6.650   Cond. No.                     2.10e+16\n",
       "==============================================================================\n",
       "\n",
       "Warnings:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "[2] The smallest eigenvalue is 5.06e-28. This might indicate that there are\n",
       "strong multicollinearity problems or that the design matrix is singular.\n",
       "\"\"\""
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = sm.OLS(sale_price, X)\n",
    "model_results = model.fit()\n",
    "model_results.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So as we can see, some of the OverallQual and OverallCond are related to the year the house was built!  This appears clear from the size of the coefficent's on those two variables now as apposed to before.  Let's look at the correlations between these variables and the categorical variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(-0.41356191390627833, 2.067718955357991e-61)\n",
      "(-0.1201943389693286, 4.1169278356547164e-06)\n",
      "(-0.24378009535094253, 3.3959083030753634e-21)\n",
      "(0.5703948291588168, 9.14784007189097e-127)\n",
      "(0.25438869055054214, 5.35693992261364e-23)\n",
      "\n",
      "(0.19731322118479108, 2.788423398821179e-14)\n",
      "(0.16135719004591492, 5.616132486368786e-10)\n",
      "(0.1103711874771551, 2.3726607477861606e-05)\n",
      "(-0.32511370797131084, 2.736066439219743e-37)\n",
      "(-0.10006196270827494, 0.00012829835928363485)\n"
     ]
    }
   ],
   "source": [
    "print(stats.pearsonr(train[\"OverallQual\"], train[\"reasonably_old\"]))\n",
    "print(stats.pearsonr(train[\"OverallQual\"], train[\"really_old\"]))\n",
    "print(stats.pearsonr(train[\"OverallQual\"], train[\"a_little_old\"]))\n",
    "print(stats.pearsonr(train[\"OverallQual\"], train[\"somewhat_new\"]))\n",
    "print(stats.pearsonr(train[\"OverallQual\"], train[\"brand_new\"]))\n",
    "print()\n",
    "print(stats.pearsonr(train[\"OverallCond\"], train[\"reasonably_old\"]))\n",
    "print(stats.pearsonr(train[\"OverallCond\"], train[\"really_old\"]))\n",
    "print(stats.pearsonr(train[\"OverallCond\"], train[\"a_little_old\"]))\n",
    "print(stats.pearsonr(train[\"OverallCond\"], train[\"somewhat_new\"]))\n",
    "print(stats.pearsonr(train[\"OverallCond\"], train[\"brand_new\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So it seems as though OverallQual is actually a strongly related to age of the house.  Let's see what happens if we take out OverallQual and leave in reasonably_old, a_little_old, somewhat_new and brand_new.\n",
    "\n",
    "As an aside, interestingly the effects are reversed for quality and condition!  This is surprising!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"log_SalePrice\"] = np.log(train[\"SalePrice\"])\n",
    "train[\"log_LotArea\"] = np.log(train[\"LotArea\"])\n",
    "\n",
    "sale_price = train[\"log_SalePrice\"]\n",
    "X = train[[\"log_LotArea\", \n",
    "           \"reasonably_old\", \n",
    "           \"a_little_old\", \n",
    "           \"somewhat_new\", \n",
    "           \"brand_new\"]\n",
    "         ]\n",
    "X = sm.add_constant(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>      <td>log_SalePrice</td>  <th>  R-squared:         </th> <td>   0.548</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.546</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   352.6</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Wed, 09 May 2018</td> <th>  Prob (F-statistic):</th> <td>1.14e-247</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>07:50:40</td>     <th>  Log-Likelihood:    </th> <td> -151.67</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>  1460</td>      <th>  AIC:               </th> <td>   315.3</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>  1454</td>      <th>  BIC:               </th> <td>   347.1</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     5</td>      <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "         <td></td>           <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th>          <td>    9.0580</td> <td>    0.128</td> <td>   70.967</td> <td> 0.000</td> <td>    8.808</td> <td>    9.308</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>log_LotArea</th>    <td>    0.2980</td> <td>    0.014</td> <td>   21.866</td> <td> 0.000</td> <td>    0.271</td> <td>    0.325</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>reasonably_old</th> <td>   -0.0170</td> <td>    0.032</td> <td>   -0.536</td> <td> 0.592</td> <td>   -0.079</td> <td>    0.045</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>a_little_old</th>   <td>    0.1404</td> <td>    0.032</td> <td>    4.410</td> <td> 0.000</td> <td>    0.078</td> <td>    0.203</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>somewhat_new</th>   <td>    0.5185</td> <td>    0.031</td> <td>   16.797</td> <td> 0.000</td> <td>    0.458</td> <td>    0.579</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>brand_new</th>      <td>    0.7856</td> <td>    0.050</td> <td>   15.563</td> <td> 0.000</td> <td>    0.687</td> <td>    0.885</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>81.008</td> <th>  Durbin-Watson:     </th> <td>   1.943</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td> <th>  Jarque-Bera (JB):  </th> <td> 266.074</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td>-0.178</td> <th>  Prob(JB):          </th> <td>1.67e-58</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 5.061</td> <th>  Cond. No.          </th> <td>    169.</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:          log_SalePrice   R-squared:                       0.548\n",
       "Model:                            OLS   Adj. R-squared:                  0.546\n",
       "Method:                 Least Squares   F-statistic:                     352.6\n",
       "Date:                Wed, 09 May 2018   Prob (F-statistic):          1.14e-247\n",
       "Time:                        07:50:40   Log-Likelihood:                -151.67\n",
       "No. Observations:                1460   AIC:                             315.3\n",
       "Df Residuals:                    1454   BIC:                             347.1\n",
       "Df Model:                           5                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==================================================================================\n",
       "                     coef    std err          t      P>|t|      [0.025      0.975]\n",
       "----------------------------------------------------------------------------------\n",
       "const              9.0580      0.128     70.967      0.000       8.808       9.308\n",
       "log_LotArea        0.2980      0.014     21.866      0.000       0.271       0.325\n",
       "reasonably_old    -0.0170      0.032     -0.536      0.592      -0.079       0.045\n",
       "a_little_old       0.1404      0.032      4.410      0.000       0.078       0.203\n",
       "somewhat_new       0.5185      0.031     16.797      0.000       0.458       0.579\n",
       "brand_new          0.7856      0.050     15.563      0.000       0.687       0.885\n",
       "==============================================================================\n",
       "Omnibus:                       81.008   Durbin-Watson:                   1.943\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              266.074\n",
       "Skew:                          -0.178   Prob(JB):                     1.67e-58\n",
       "Kurtosis:                       5.061   Cond. No.                         169.\n",
       "==============================================================================\n",
       "\n",
       "Warnings:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = sm.OLS(sale_price, X)\n",
    "model_results = model.fit()\n",
    "model_results.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see model performance suffers significantly.  Which indicates there is an effect from age of house, but it is difficuilt for a linear model to get at.  Let's try a different tactic to get at this interaction, because there is certainly something here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiplying variables\n",
    "\n",
    "There are two many ways to do multiplication of variables - to multiple variables together or takes powers of variables.\n",
    "\n",
    "When you multiple variables together, you explicitly account for variables varying together.  So when you look at the multiplication of variables - you are answering the question, what happens when I look at the effect of x[1] and x[2]. When you look at powers of a variable, like a variable squared, you can account for diminishing returns of a variable, given that the sign on the coefficient is negative.\n",
    "\n",
    "Let's start by trying to update our model, to include a multiplication of, `OverallQual` and `really_old` and  `OverallCond` and `really_old`.\n",
    "\n",
    "`log(SalePrice) = B[0] + B[1]*log(LotArea) + B[2]*OverallQual + B[3]*OverallCond + B[4]*OverallQual*really_old + B[5]*OverallCond*really_old + u`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"log_SalePrice\"] = np.log(train[\"SalePrice\"])\n",
    "train[\"log_LotArea\"] = np.log(train[\"LotArea\"])\n",
    "train[\"really_old_OverallQual\"] = train[\"really_old\"] * train[\"OverallQual\"]\n",
    "train[\"really_old_OverallCond\"] = train[\"really_old\"] * train[\"OverallCond\"]\n",
    "\n",
    "\n",
    "sale_price = train[\"log_SalePrice\"]\n",
    "X = train[[\"log_LotArea\",\n",
    "            \"OverallQual\", \n",
    "           \"OverallCond\",\n",
    "           \"really_old_OverallQual\",\n",
    "           \"really_old_OverallCond\"]\n",
    "         ]\n",
    "X = sm.add_constant(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>      <td>log_SalePrice</td>  <th>  R-squared:         </th> <td>   0.744</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.743</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   845.5</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Wed, 09 May 2018</td> <th>  Prob (F-statistic):</th>  <td>  0.00</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>08:01:28</td>     <th>  Log-Likelihood:    </th> <td>  263.52</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>  1460</td>      <th>  AIC:               </th> <td>  -515.0</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>  1454</td>      <th>  BIC:               </th> <td>  -483.3</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     5</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "             <td></td>               <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th>                  <td>    8.7210</td> <td>    0.098</td> <td>   88.994</td> <td> 0.000</td> <td>    8.529</td> <td>    8.913</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>log_LotArea</th>            <td>    0.2042</td> <td>    0.010</td> <td>   19.611</td> <td> 0.000</td> <td>    0.184</td> <td>    0.225</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>OverallQual</th>            <td>    0.2239</td> <td>    0.004</td> <td>   56.760</td> <td> 0.000</td> <td>    0.216</td> <td>    0.232</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>OverallCond</th>            <td>    0.0151</td> <td>    0.005</td> <td>    3.018</td> <td> 0.003</td> <td>    0.005</td> <td>    0.025</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>really_old_OverallQual</th> <td>   -0.0686</td> <td>    0.015</td> <td>   -4.631</td> <td> 0.000</td> <td>   -0.098</td> <td>   -0.040</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>really_old_OverallCond</th> <td>    0.0401</td> <td>    0.013</td> <td>    3.084</td> <td> 0.002</td> <td>    0.015</td> <td>    0.066</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>249.840</td> <th>  Durbin-Watson:     </th> <td>   1.917</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td> 850.897</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td>-0.825</td>  <th>  Prob(JB):          </th> <td>1.70e-185</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 6.356</td>  <th>  Cond. No.          </th> <td>    230.</td> \n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:          log_SalePrice   R-squared:                       0.744\n",
       "Model:                            OLS   Adj. R-squared:                  0.743\n",
       "Method:                 Least Squares   F-statistic:                     845.5\n",
       "Date:                Wed, 09 May 2018   Prob (F-statistic):               0.00\n",
       "Time:                        08:01:28   Log-Likelihood:                 263.52\n",
       "No. Observations:                1460   AIC:                            -515.0\n",
       "Df Residuals:                    1454   BIC:                            -483.3\n",
       "Df Model:                           5                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==========================================================================================\n",
       "                             coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------------------\n",
       "const                      8.7210      0.098     88.994      0.000       8.529       8.913\n",
       "log_LotArea                0.2042      0.010     19.611      0.000       0.184       0.225\n",
       "OverallQual                0.2239      0.004     56.760      0.000       0.216       0.232\n",
       "OverallCond                0.0151      0.005      3.018      0.003       0.005       0.025\n",
       "really_old_OverallQual    -0.0686      0.015     -4.631      0.000      -0.098      -0.040\n",
       "really_old_OverallCond     0.0401      0.013      3.084      0.002       0.015       0.066\n",
       "==============================================================================\n",
       "Omnibus:                      249.840   Durbin-Watson:                   1.917\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              850.897\n",
       "Skew:                          -0.825   Prob(JB):                    1.70e-185\n",
       "Kurtosis:                       6.356   Cond. No.                         230.\n",
       "==============================================================================\n",
       "\n",
       "Warnings:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = sm.OLS(sale_price, X)\n",
    "model_results = model.fit()\n",
    "model_results.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Features from Exogenous data\n",
    "\n",
    "So far we've looked at how to make your models better by transforming the data you have.  But one of the most important kinds of feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
