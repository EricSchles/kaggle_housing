{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/statsmodels/compat/pandas.py:56: FutureWarning: The pandas.core.datetools module is deprecated and will be removed in a future version. Please use the pandas.tseries module instead.\n",
      "  from pandas.core import datetools\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn import linear_model\n",
    "from sklearn import svm\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt\n",
    "import skfeature\n",
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# An Introduction to Feature Engineering\n",
    "\n",
    "By Eric Schles\n",
    "\n",
    "Today we are going to cover feature engineering.  We'll start with the easiest form of feature engineering - creating dummy variables.  And then we'll move onto taking the log of diffierent variables, taking the variable and the square of variables, adding the results of other models as features to the model, and finally automatic feature selection, via skfeature and sklearn.  \n",
    "\n",
    "To summarize we'll cover:\n",
    "\n",
    "* dummy variables\n",
    "* logs of variables\n",
    "* multiplying variables\n",
    "* the outputs of other models as features\n",
    "* automatic feature selection via skfeature\n",
    "* automatic feature selection via PCA\n",
    "* automatic feature selection via TSNE\n",
    "* automatic feature selection via DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first let's import some data\n",
    "\n",
    "train = pd.read_csv(\"train.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A brief introduction to Dummy variables\n",
    "\n",
    "A dummy variable is a transformation of a categorical variable into a numeric variable.  This allows us to process a whole new set of data points, that we couldn't before.  However, there are some dangers.  We shouldn't interpret dummy variables as truly numeric, because there isn't necessarily a meaning to the transformation we choose.  As long as we map to variables that don't have large magnitudinal differences, and there are only a few categories, dummy variables give us a lot of power.\n",
    "\n",
    "But if we have a ton of categories, a strict dummy variable mapping may be ill advised."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pave    1454\n",
       "Grvl       6\n",
       "Name: Street, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# starting with dummy variables\n",
    "# We'll want to pick dummary variables that occur a somewhat balanced number of times\n",
    "# that makes variables like the following poor choices:\n",
    "\n",
    "train[\"Street\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reg    925\n",
       "IR1    484\n",
       "IR2     41\n",
       "IR3     10\n",
       "Name: LotShape, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# And variables like LotShape pretty good:\n",
    "train[\"LotShape\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Operationalizing dummy variables\n",
    "\n",
    "Now that we've figured out what a good dummy variable looks like, let's operationalize it with a function!  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dummy_variables(df, length_cut_off_percent = 0.75, max_concentration = 0.65):\n",
    "    candidate_columns = []\n",
    "    for column in df.columns:\n",
    "        if df[column].dtype == object:\n",
    "            candidate_columns.append(column)\n",
    "    \n",
    "    for column in candidate_columns:\n",
    "        value_counts = df[column].value_counts()\n",
    "        if len(value_counts) < len(df)*length_cut_off_percent: \n",
    "            candidate_columns.remove(column)\n",
    "            continue\n",
    "        sum_vals = sum(value_counts)\n",
    "        percentages = [elem/sum_vals > max_concentration for elem in value_counts]\n",
    "        if any(percentages):\n",
    "            candidate_columns.remove(column)\n",
    "        continue\n",
    "    \n",
    "    for column in candidate_columns:\n",
    "        dummy_columns = pd.get_dummies(df[column])\n",
    "        df = pd.concat([df, dummy_columns], axis=1)\n",
    "        df.drop(column, axis=1, inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1460, 83) (1460, 175)\n"
     ]
    }
   ],
   "source": [
    "# now let's see which variables we are going to turn into dummies\n",
    "train_with_dummies = generate_dummy_variables(train)\n",
    "print(train.shape, train_with_dummies.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see we went from 83 columns to 175.  That's a lot more features!  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logarithmic transformation\n",
    "\n",
    "I'm not sure if logarithmic transformations are considered \"feature engineering\" however they are a powerful tool used often in econometrics to bring flexibility to the modeling of data.  In order to make use of and interpret this next technique we will need some information about our dataset.  Does it makes sense to take the log of any of the variables?\n",
    "\n",
    "One of the ways you can tell this, is if the dependent variable and the independent variables are not in the same scale.  Which is often the case when dealing with prices.  So let's go ahead and build a rather standard model with our data:\n",
    "\n",
    "`log(SalePrice) = B[0] + B[1]*log(LotArea) + B[2]*log(BedroomAbvGr) + B[3]*log(ExterQual) + B[4]*log(ExterCond) + u`\n",
    "\n",
    "```\n",
    "SalePrice = the price the house was sold for \n",
    "LotArea = the size of the lot\n",
    "BedroomAbvGr = the number of bedrooms above basement level\n",
    "ExterQual = the quality of the exterior of the home (See Note)\n",
    "ExterCond = the condition of the exterior of the home (See Note)\n",
    "```\n",
    "\n",
    "Note - Even though these are categorical variables, there is an ordering to each variable Excellent, Good, Average, Fair and Poor.  So we can translate these variables into numeric ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping = {\n",
    "    \"Ex\": 5, # Excellent\n",
    "    \"Gd\": 4, # Good\n",
    "    \"TA\": 3, # Average/Typical\n",
    "    \"Fa\": 2, # Fair\n",
    "    \"Po\": 1 # Poor\n",
    "}\n",
    "train[\"ExterQual\"] = train[\"ExterQual\"].map(mapping)\n",
    "train[\"ExterCond\"] = train[\"ExterCond\"].map(mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:3: RuntimeWarning: divide by zero encountered in log\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "train[\"log_SalePrice\"] = np.log(train[\"SalePrice\"])\n",
    "train[\"log_LotArea\"] = np.log(train[\"LotArea\"])\n",
    "train[\"log_BedroomAbvGr\"] = np.log(train[\"BedroomAbvGr\"])\n",
    "train[\"log_ExterQual\"] = np.log(train[\"ExterQual\"])\n",
    "train[\"log_ExterCond\"] = np.log(train[\"ExterCond\"])\n",
    "\n",
    "sale_price = train[\"log_SalePrice\"]\n",
    "X = train[[\"log_LotArea\", \"log_BedroomAbvGr\", \"log_ExterQual\",\"log_ExterCond\"]]\n",
    "X = sm.add_constant(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = sm.OLS(sale_price, X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_results = model.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_results.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpretting our results\n",
    "\n",
    "Generally speaking when one applies a log to both sides of an OLS model, we are asking \"What is the elasticity of sale price with respect to lot area, number of above ground bedrooms, external quality of the material of the house and external condition of the house?\"  Thinking of this another way, we can see this as the percentage change in price with respect to a percentage change in a dependent variable.  \n",
    "\n",
    "Thus, we have put both sides of our equation in relative, not absolute terms.  In terms of a percentage change, things can be misleading, if not well understood.  But when treated carefully, percentage changes can shed more light than just dealing with absolute numbers.  Especially when changes in Y appear insensitive to changes in X, due to scaling reasons.  \n",
    "\n",
    "This that in mind, let's look at our results!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
